{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this expercise, we will utilize two files (listed below) from the 'Brazilian E-Commerce Public Dataset' originally posted on [Kaggle]( https://www.kaggle.com/olistbr/brazilian-ecommerce)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full path of the 'orders' dataset\n",
    "orders_file = 'https://bitbucket.org/vishal_derive/vcu-data-mining/raw/11253d9f443241b3ce5949802966a80de73af1db/data/olist_orders_dataset.csv'\n",
    "\n",
    "# full path of the 'customer' dataset\n",
    "cust_file = 'https://bitbucket.org/vishal_derive/vcu-data-mining/raw/11253d9f443241b3ce5949802966a80de73af1db/data/olist_customers_dataset.csv'\n",
    "\n",
    "payments_file = 'https://bitbucket.org/vishal_derive/vcu-data-mining/raw/aedab41b6b30a19db9c72e5b82755a118f847d87/data/olist_order_payments_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_olist_data(file1, file2, file3, verbose):\n",
    "    \n",
    "    # read the data\n",
    "    orders = pd.read_csv(file1)\n",
    "    cust = pd.read_csv(file2)\n",
    "    payments = pd.read_csv(file3)\n",
    "\n",
    "    # drop unnecessary columns\n",
    "    #drop_vars = ['order_approved_at', 'order_delivered_carrier_date', \n",
    "    #           'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "\n",
    "    \n",
    "    # date-time conversion\n",
    "    orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "    orders['order_delivered_customer_timestamp'] = pd.to_datetime(orders['order_delivered_customer_date'])\n",
    "    orders['order_estimated_delivery_timestamp'] = pd.to_datetime(orders['order_estimated_delivery_date'])\n",
    "\n",
    "    # let's convert the order purchase timestamps into dates\n",
    "    orders['order_purchase_date'] = orders['order_purchase_timestamp'].dt.date\n",
    "    orders['order_delivered_customer_formdate'] = orders['order_delivered_customer_timestamp'].dt.date\n",
    "    orders['order_estimated_delivery_formdate'] = orders['order_estimated_delivery_timestamp'].dt.date\n",
    "\n",
    "     # extract day of week from the order date\n",
    "    orders['order_dow'] = orders['order_purchase_timestamp'].dt.weekday_name\n",
    "    \n",
    "    # extract month from the order date\n",
    "    orders['order_month'] = orders['order_purchase_timestamp'].dt.month\n",
    "\n",
    "    \n",
    "    # merge orders and cust dataframes\n",
    "    orders_cust = pd.merge(orders, cust, on='customer_id', how='inner')\n",
    "    orders_out = pd.merge(orders_cust, payments, on='order_id', how='inner')\n",
    "    \n",
    "    # apply filters to (a) discard (incomplete) data after 2018-8-22; see 06_pandas_wrangle.ipynb for the rationale\n",
    "    #  and (b) keep 'delivered' orders only\n",
    "    #  we do this here by using a boolean (True/False) mask\n",
    "    mask = (orders_out['order_purchase_date'] <= date(2018, 8, 22)) & (orders_out['order_status'] == 'delivered')\n",
    "\n",
    "    orders_out = orders_out[mask]\n",
    "    \n",
    "    # discard 'order_status' as we don't need it any more\n",
    "    orders_out = orders_out.drop('order_status', axis=1)\n",
    "    \n",
    "    # let's drop hose columns that we need (for this exercise)\n",
    "    keep_cols = ['customer_unique_id', 'customer_id','order_id','order_purchase_timestamp', 'order_delivered_customer_timestamp',\n",
    "                 'order_estimated_delivery_timestamp', 'order_purchase_date', 'order_delivered_customer_formdate', 'order_estimated_delivery_formdate',\n",
    "                 'order_dow', 'order_month', 'payment_installments', 'payment_type', 'payment_value']\n",
    "\n",
    "    orders_out = orders_out[keep_cols].sort_values(['customer_unique_id', 'order_purchase_timestamp'])\n",
    "\n",
    "    #if verbose:\n",
    "       # print (f'{len(orders_out):,} records in the output  file.')\n",
    "    \n",
    "    return orders_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = read_olist_data(orders_file, cust_file, payments_file, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "payments = pd.read_csv(payments_file)\n",
    "#payments[payments['order_id']=='31bc09fdbd701a7a4f9b55b5955b8687']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model to predict whether a customer will make a purchase within the next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our predictin window is July 2017 through end of June 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_date = date(2017, 6, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100247, 15077)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filter out only data before snapshot date\n",
    "mask = orders['order_purchase_timestamp'].dt.date <= snapshot_date\n",
    "\n",
    "df_raw = orders[mask]\n",
    "\n",
    "len(orders), len(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute #1: Order recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13855"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# check the max date\n",
    "\n",
    "df_raw.order_purchase_timestamp.max()\n",
    "\n",
    "# grab the max purchase date for each customer\n",
    "\n",
    "cust_recency = df_raw.groupby('customer_unique_id')['order_purchase_timestamp'].max().reset_index()\n",
    "\n",
    "# count the number of days between the most recent order date and the snapshot date\n",
    "\n",
    "delta = snapshot_date - cust_recency['order_purchase_timestamp'].dt.date\n",
    "\n",
    "# grab the number of days (as an integer) from the calculated deltas \n",
    "\n",
    "cust_recency['order_recency'] = delta.dt.days\n",
    "\n",
    "# drop the date (we don't need it any more for this exercise)\n",
    "\n",
    "cust_recency = cust_recency.drop('order_purchase_timestamp', axis=1)\n",
    "\n",
    "\n",
    "# how many records (unique customers) do we have for this timeframe?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute 2: Days defore/after delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liz/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# count the number of days between the most recent order date and the snapshot date\n",
    "\n",
    "df_raw['delays'] = (df_raw['order_estimated_delivery_timestamp'].dt.date - df_raw['order_delivered_customer_timestamp'].dt.date).dt.days\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13855"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_delay = df_raw.groupby(['customer_unique_id'])['delays'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute 3: Orders per Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13855"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count total records (i.e., orders) per customer\n",
    "\n",
    "cust_orders = df_raw.groupby('customer_unique_id').size().reset_index().rename(columns={0: 'total_orders'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ### Dummy attributes 4, 5 and 6: Payment Type, Day of Week, Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13855, 13855, 13855)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dummies\n",
    "\n",
    "df_raw = pd.get_dummies(df_raw, columns=['payment_type','order_dow','order_month'])\n",
    "\n",
    "# grab all binary columns that we just created and aggregate at the customer level\n",
    "\n",
    "#payment type\n",
    "dumpay = [x for x in df_raw.columns if 'payment_type_' in x]\n",
    "payment_type = df_raw.groupby('customer_unique_id')[dumpay].sum().reset_index()\n",
    "\n",
    "# day of week\n",
    "dumdow = [x for x in df_raw.columns if 'order_dow_' in x]\n",
    "cust_dow = df_raw.groupby('customer_unique_id')[dumdow].sum().reset_index()\n",
    "\n",
    "# month\n",
    "dummonth = [x for x in df_raw.columns if 'order_month_' in x]\n",
    "cust_month = df_raw.groupby('customer_unique_id')[dummonth].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute 7: Purchase Dollar Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13855"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['order_total']=df_raw['payment_installments']*df_raw['payment_value']\n",
    "\n",
    "dollar_volume = df_raw.groupby(['customer_unique_id'])['order_total'].sum().reset_index().sort_values('order_total',ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all attributes into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13855, 2),\n",
       " (13855, 2),\n",
       " (13855, 8),\n",
       " (13855, 2),\n",
       " (13855, 9),\n",
       " (13855, 2),\n",
       " (13855, 5))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# double check the number of records in each dataframe first\n",
    "\n",
    "cust_recency.shape, cust_orders.shape, cust_dow.shape, order_delay.shape, cust_month.shape, dollar_volume.shape, payment_type.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13855, 24)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine (merge) `cust_recency` with `cust_orders` so that we have both attributes in one dataset\n",
    "\n",
    "df = pd.merge(cust_recency, cust_orders, on='customer_unique_id') \\\n",
    "     .merge(cust_dow, on='customer_unique_id') \\\n",
    "     .merge(order_delay, on='customer_unique_id') \\\n",
    "     .merge(cust_month, on='customer_unique_id') \\\n",
    "     .merge(dollar_volume, on='customer_unique_id') \\\n",
    "     .merge(payment_type, on='customer_unique_id') \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign labels (aka the target variable or the dependent variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will assume that the objective of the model is to predict whether a customer will make *at least one purchase* in the future (i.e., within the target window of the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Isolate all orders that were placed within the prediction window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72774"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# select orders that were placed between July 2017 and 2018\n",
    "\n",
    "mask = (orders['order_purchase_timestamp'].dt.date > snapshot_date) & (orders['order_purchase_timestamp'].dt.date < date(2018,7,1))\n",
    "target_events_raw = orders[mask]\n",
    "\n",
    "len(target_events_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-01 00:04:15 2018-06-30 23:59:49\n"
     ]
    }
   ],
   "source": [
    "# confirm the min and max dates are within the month of August 2018\n",
    "\n",
    "print (target_events_raw['order_purchase_timestamp'].min(), target_events_raw['order_purchase_timestamp'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Summarize data to get one record per customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>purch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000366f3b9a7992bf8c76cfdf3221e2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000b849f77a49e4a4ce2b2a4ca5be3f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000f6ccb0745a6a4b88665a16c9f078</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004aac84e0df4da2b147fca70cf8255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004bd2a26a76fe21f786e4fbd80607f</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 customer_unique_id  purch\n",
       "0  0000366f3b9a7992bf8c76cfdf3221e2      1\n",
       "1  0000b849f77a49e4a4ce2b2a4ca5be3f      1\n",
       "2  0000f6ccb0745a6a4b88665a16c9f078      1\n",
       "3  0004aac84e0df4da2b147fca70cf8255      1\n",
       "4  0004bd2a26a76fe21f786e4fbd80607f      1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of orders (we will convert this into a binary flag later)\n",
    "\n",
    "target_events = target_events_raw.groupby('customer_unique_id').size().reset_index().rename(columns={0: 'purch'})\n",
    "\n",
    "target_events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of customers who made at least one purchase durnig the prediction window: 67,776\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of customers who made at least one purchase durnig the prediction window: {len(target_events):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Merge this dataframe with the `attr` dataframe to create the modeling dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13855, 25)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge `target_events` with the dataframe that contains customer attributes\n",
    "\n",
    "df = pd.merge(df, target_events, how='left', on='customer_unique_id')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13488\n",
       "1      367\n",
       "Name: purch, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the binary target flag\n",
    "\n",
    "df['purch'] = [1 if x > 0 else 0 for x in df['purch']]\n",
    "\n",
    "df.purch.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.973511\n",
       "1    0.026489\n",
       "Name: purch, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % distribution of the target flag\n",
    "\n",
    "df.purch.value_counts() / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the prediction window helped some, but the target event (1 for purch) is still rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_recency              -0.026816\n",
       "total_orders                0.005163\n",
       "order_dow_Friday           -0.006783\n",
       "order_dow_Monday           -0.011389\n",
       "order_dow_Saturday          0.006374\n",
       "order_dow_Sunday           -0.000218\n",
       "order_dow_Thursday          0.011081\n",
       "order_dow_Tuesday          -0.004688\n",
       "order_dow_Wednesday         0.013004\n",
       "delays                      0.016203\n",
       "order_month_1               0.002737\n",
       "order_month_2              -0.021879\n",
       "order_month_3              -0.010577\n",
       "order_month_4              -0.007809\n",
       "order_month_5               0.012584\n",
       "order_month_6               0.021171\n",
       "order_month_10              0.004326\n",
       "order_month_12             -0.001401\n",
       "order_total                 0.009565\n",
       "payment_type_boleto        -0.020327\n",
       "payment_type_credit_card    0.027989\n",
       "payment_type_debit_card    -0.002468\n",
       "payment_type_voucher       -0.001802\n",
       "purch                       1.000000\n",
       "Name: purch, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the correlations\n",
    "\n",
    "df.corr()['purch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "\n",
    "preds = df.columns[1:-1]\n",
    "\n",
    "X = df[preds]\n",
    "y = df['purch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-fold partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6927, 6928)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataframe into train(50%) and test(50%)\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=314)\n",
    "\n",
    "#len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset  \n",
    "shuffle_df = df.sample(frac=1)\n",
    "\n",
    "# Define a size for your train set \n",
    "train_size = int(0.5* len(df))\n",
    "\n",
    "# Split your dataset \n",
    "train_set = shuffle_df[:train_size]\n",
    "test_set = shuffle_df[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data without using sklearn function. Not recommended since \n",
    "#we cannot specify a random seed, and thus cannot duplicate exact results.\n",
    "X_train=train_set[['order_recency', 'total_orders',\n",
    "       'order_dow_Friday', 'order_dow_Monday', 'order_dow_Saturday',\n",
    "       'order_dow_Sunday', 'order_dow_Thursday', 'order_dow_Tuesday',\n",
    "       'order_dow_Wednesday']]\n",
    "y_train = train_set['purch']\n",
    "\n",
    "X_test=test_set[['order_recency', 'total_orders',\n",
    "       'order_dow_Friday', 'order_dow_Monday', 'order_dow_Saturday',\n",
    "       'order_dow_Sunday', 'order_dow_Thursday', 'order_dow_Tuesday',\n",
    "       'order_dow_Wednesday']]\n",
    "y_test = test_set['purch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define the model object ('liblinear' is recommended for small datasets)\n",
    "clf = LogisticRegression(solver='liblinear', random_state=314)\n",
    "\n",
    "# train (fit) the model using the training sample\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test sample\n",
    "y_preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The parameters added to the decision tree model to reduce overfitting improved the model's performance\n"
     ]
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "dtree = DecisionTreeClassifier(max_depth=4, min_samples_split = 0.05, min_samples_leaf = 0.02, random_state=314)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dtree = dtree.fit(X_train,y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred_tree = dtree.predict(X_test)\n",
    "\n",
    "print(\"The parameters added to the decision tree model to reduce overfitting improved the model's performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of Logistic Regression: 0.5351371175531571\n",
      "AUC of Decision Tree: 0.5530515760775223\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# calculate the probabilities on the test sample\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calcualte AUC\n",
    "log_auc=roc_auc_score(y_test, y_scores)\n",
    "\n",
    "# calculate the probabilities on the test sample\n",
    "y_scores_tree = dtree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# calcualte AUC\n",
    "dtree_auc = roc_auc_score(y_test, y_scores_tree)\n",
    "\n",
    "print(\"AUC of Logistic Regression:\",log_auc),\n",
    "print(\"AUC of Decision Tree:\",dtree_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6752</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0  1\n",
       "purch         \n",
       "0      6752  2\n",
       "1       174  0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion Matrix for Logistic Regression\n",
    "confusion_log = pd.crosstab(y_test, y_preds)\n",
    "confusion_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is predicting 2 purchases for all customers. However, it did not accurately predict any purchases. The accuracy score is still  not a reliable measure because the target event is still relatively rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6747</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0  1\n",
       "purch         \n",
       "0      6747  7\n",
       "1       173  1"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion Matrix for Decision Tree\n",
    "confusion_tree = pd.crosstab(y_test, y_pred_tree)\n",
    "confusion_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.9737086297556449\n",
      "Accuracy of Decision Tree: 0.9730900092793071\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy of Logistic Regression:\",accuracy_score(y_test, y_preds)),\n",
    "print(\"Accuracy of Decision Tree:\",accuracy_score(y_test, y_pred_tree))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Model Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.031\n",
      "Accuracy of Decision Tree: 0.029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "average_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "average_precision_tree = average_precision_score(y_test, y_scores_tree)\n",
    "\n",
    "print(\"Accuracy of Logistic Regression:\",average_precision.round(3)),\n",
    "print(\"Accuracy of Decision Tree:\",average_precision_tree.round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Recall Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score for Logistic Regression: 0.0\n",
      "Recall Score for Decision Tree: 0.005747126436781609\n"
     ]
    }
   ],
   "source": [
    "#Recall score is the ratio of true positives to all positive values. \n",
    "#In other words, of all actual positive values, what percent are accurately\n",
    "#precicted by the model\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "print(\"Recall Score for Logistic Regression:\",recall_score(y_test, y_preds)),\n",
    "print(\"Recall Score for Decision Tree:\",recall_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Misclassification Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassification for Logistic Regression: 0.025404157043879907\n",
      "Misclassification for Decision Tree: 0.025981524249422634\n"
     ]
    }
   ],
   "source": [
    "print(\"Misclassification for Logistic Regression:\",(confusion_log.loc[1,0]+confusion_log.loc[0,1])\\\n",
    "/(confusion_log.sum()[1]+confusion_log.sum()[0])),\n",
    "print(\"Misclassification for Decision Tree:\",(confusion_tree.loc[1,0]+confusion_tree.loc[0,1])\\\n",
    "/(confusion_tree.sum()[1]+confusion_tree.sum()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911498646659971"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf.predict_proba(X_test)[:,1].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
